window.projects = [
  {
    "id": "spectral-subjects",
    "title": "SPECTRAL SUBJECTS",
    "year": null,
    "client": "Rafael Lozano-Hemmer / MOCA Jacksonville",
    "role": null,
    "technologies": null,
    "description": "Thermographic cameras, projection mapping, Développement d'une carte thermique interactive de l'espace",
    "impact": null,
    "impact-title": "SPECTRAL SUBJECTS — Turning infrared heat into live, data-driven portraiture",
    "media": [
      "https://www.instagram.com/lozanohemmer/"
    ],
    "instagram": [
      "https://www.instagram.com/lozanohemmer/"
    ],
    "cover": null,
    "hasMedia": true,
    "fullDescription": "Spectral Subjects is a thermal observatory that makes invisible heat exchanges visible. High-sensitivity infrared cameras map the room’s temperature in real time, showing visitors’ bodies, breath, and movements as flowing fields of particles. The work blends live thermal data with echoes of the recent past, layering traces so the atmosphere itself becomes a kind of portrait. Presented both as large-scale projections and as screen-based editions, the piece adapts to its setting while keeping interaction immediate and intuitive. Exhibited at MOCA Jacksonville in 2024–25, Spectral Subjects invited the public to see themselves as part of a constantly evolving thermal landscape.**Technical details and implementation**High-sensitivity thermographic cameras (Xenics Dione class) capture temperature variations in real-timeCustom GPU processing transforms thermal data into flowing particle systems with temporal persistenceLarge-scale architectural projection calibrated for the atrium's unique surfaces and lighting conditionsMultiple editions: immersive projection version and intimate screen-based installations**Challenges and solutions**Complex thermal backgrounds required per-site calibration and adaptive threshold algorithmsAchieving responsive interaction demanded optimized GPU pipelines with minimal latencyBalancing signal clarity with ambient noise through temporal filteringArchitectural projection mapping required precise geometric calibration and keystone correction**Impact and results**Makes the invisible exchanges of daily life—heat, breath, presence—visually tangibleVisitors become co-authors of the workExhibited Dec 2024–Jun 2025 at MOCA Jacksonville as part of centennial programmingGenerated exceptional public engagement through intuitive, immediate interaction**Process and methodology**Comprehensive site analysis including thermal mapping, HVAC patterns, and lighting studiesIterative prototyping of diffusion algorithms and visual palettesOn-site installation with continuous tuning based on real visitor behavior patterns---",
    "videoEmbed": "https://www.youtube.com/watch?v=zM14-yzc_hI"
  },
  {
    "id": "climate-parliament",
    "title": "CLIMATE PARLIAMENT",
    "year": null,
    "client": "Rafael Lozano-Hemmer / Rice University",
    "role": null,
    "technologies": null,
    "description": "Audio-reactive lighting, interactive sound installation, Développement d'une installation sonore et lumineuse interactive",
    "impact": null,
    "impact-title": "CLIMATE PARLIAMENT — Orchestrating 480+ speaker-lights into a responsive climate soundscape",
    "media": [
      "https://www.instagram.com/lozanohemmer/"
    ],
    "instagram": [
      "https://www.instagram.com/lozanohemmer/"
    ],
    "cover": null,
    "hasMedia": true,
    "fullDescription": "Climate Parliament is an immersive installation where 481 suspended speaker-lights form a responsive canopy of sound and light. Archival recordings, scientific testimonies, protest chants, and personal reflections are distributed across the array, activating as visitors move through the space. Each step assembles temporary coalitions of voices, turning abstract climate discourse into an embodied experience. Sophisticated spatial audio processing prevents overload, ensuring clarity even as thousands of clips overlap. By walking among the lights, visitors physically gather and disperse debates, experiencing climate voices as a living parliament.\n**Technical details and implementation**\n- Suspended array of 481 pendant speaker-lights with integrated LED rings creating a responsive canopy\n- Thousands of curated audio clips distributed across nodes with sophisticated spatial audio processing\n- Advanced presence and motion sensing triggers localized activations and wave-propagation patterns\n- Real-time audio mixing prevents sonic chaos while maintaining spatial clarity\n**Challenges and solutions**\n- Preventing sonic overload required spatial mixing algorithms, dynamic range control, and per-zone prioritization\n- Managing power, data, and synchronization across hundreds of nodes demanded robust network topology and precise clocking\n- Eliminating false triggers and dead zones required carefully tuned sensor thresholds and comprehensive coverage mapping\n**Impact and results**\n- Transforms climate discourse from abstract to embodied—visitors physically assemble debates through movement\n- Creates a unique space for reflection and discussion in public and academic contexts\n- Demonstrates how technology can make complex social issues spatially and emotionally accessible\n**Process and methodology**\n- Extensive content research and rights clearance with systematic taxonomy of sources and thematic tags\n- Scale modeling and simulation of array geometry, audio density, and decay patterns\n- On-site commissioning with iterative balancing of light and sound to achieve optimal visitor experience\n---",
    "videoEmbed": "https://www.lozano-hemmer.com/climate_parliament.php"
  },
  {
    "id": "field-atmosphonia",
    "title": "FIELD ATMOSPHONIA",
    "year": null,
    "client": "Rafael Lozano-Hemmer / Artis – Naples",
    "role": null,
    "technologies": null,
    "description": "Audio-reactive lighting, immersive sound environment, Développement d'un environnement sonore immersif",
    "impact": null,
    "impact-title": "FIELD ATMOSPHONIA — Composing at architectural scale with thousands of single-channel speakers",
    "media": [
      "https://www.instagram.com/lozanohemmer/"
    ],
    "instagram": [
      "https://www.instagram.com/lozanohemmer/"
    ],
    "cover": null,
    "hasMedia": true,
    "fullDescription": "Field Atmosphonia is an immersive sound environment composed of thousands of discrete audio channels spread across a vast speaker array. Instead of a fixed composition, the system redistributes attention and density in response to audience movement, making the space itself feel alive. Multiple 2D lidars were merged to track people across the entire room, allowing the system to respond seamlessly to shifts in presence and proximity. Visitors don’t just listen—they become participants in a shifting acoustic ecosystem, where their movements continually reshape the mix. The result is less a performance than an atmosphere\n**Technical details and implementation**\n- 3,000+ channel audio library distributed across multiple speakers with sophisticated spatialization\n- Interactive logic modulates density and emphasis based on real-time audience presence and movement\n- Subtle light elements serve as sonic visualizers, creating visual echoes of the audio experience\n- Advanced audio routing and content management system handles the massive channel count\n**Challenges and solutions**\n- Managing thousands of audio channels required distributed architecture and intelligent content management\n- Preventing perceptual muddiness demanded careful spectral curation and precise spatial separation\n- Ensuring consistent coverage across varied listening positions required extensive room-specific tuning\n**Impact and results**\n- Fundamentally reframes sound as environmental experience rather than performance\n- Generates prolonged dwell times and repeat visits as visitors discover new sonic relationships\n- Achieves accessibility without explicit instructions—visitors intuitively understand they're part of the mix\n**Process and methodology**\n- Iterative curation of recordings and textures to create rich, layered soundscapes\n- Small-array prototyping and testing before scaling to full installation\n- Room-specific equalization and delay compensation for optimal acoustic experience\n---",
    "videoEmbed": "https://vimeo.com/932614860"
  },
  {
    "id": "kristallstimmen",
    "title": "KRISTALLSTIMMEN",
    "year": null,
    "client": "Rafael Lozano-Hemmer / Swarovski Crystal Worlds",
    "role": null,
    "technologies": null,
    "description": "Interactive audio installation, Développement d'une installation audio interactive",
    "impact": null,
    "impact-title": "KRISTALLSTIMMEN — Activating 3,000 crystal-clad speakers with per-visitor audio light states",
    "media": [
      "https://www.instagram.com/lozanohemmer/"
    ],
    "instagram": [
      "https://www.instagram.com/lozanohemmer/"
    ],
    "cover": null,
    "hasMedia": true,
    "fullDescription": "Kristallstimmen is a permanent interactive installation created in 2024 for Swarovski’s Crystal Worlds Museum in Wattens, Austria. The work features an array of 3,000 suspended loudspeakers, each clad in hundreds of small black crystals that illuminate when active. As visitors walk through the space, sensors detect their presence and trigger the loudspeaker directly above them. A light turns on, and a voice is heard: a recording from one of Swarovski’s employees around the world. Together, the array represents over 100 languages and countless perspectives—designers, engineers, welders, researchers, salespeople, technicians, and more. When no one is present, the installation falls silent, existing as a social sculpture that shifts attention from the product to the people whose work brings it to life.\n**Technical details and implementation**\n- Microphones and speakers embedded within crystal structures create seamless audio integration\n- Advanced DSP processing generates crystalline echoes, spectral shimmers, and harmonized voice layers\n- Audio-reactive lighting system responds to sound with refracted patterns across faceted elements\n- Sophisticated feedback prevention and gain staging for close-proximity audio components\n**Challenges and solutions**\n- Highly reflective crystal environment required targeted EQ, notch filters, and strategic absorptive treatments\n- Managing feedback risk with close microphone/speaker placement demanded precise gating, directionality, and gain staging\n- Achieving clear audio in reflective space required extensive acoustic modeling and material studies\n**Impact and results**\n- Creates profound sense of wonder and self-awareness as visitors discover they can 'play' the room\n- Successfully bridges luxury brand aesthetics with cutting-edge experiential media art\n- Generates extended visitor engagement as people explore the acoustic possibilities\n**Process and methodology**\n- Comprehensive material and acoustic studies of crystal surfaces and their reflective properties\n- Rapid DSP prototyping with extensive preset banks for tonal variety and musical exploration\n- On-site voicing and calibration with live audience testing to optimize the interactive experience\n---",
    "videoEmbed": "https://vimeo.com/1051929102"
  },
  {
    "id": "shadow-tuner",
    "title": "SHADOW TUNER",
    "year": null,
    "client": "Rafael Lozano-Hemmer / Abu Dhabi",
    "role": null,
    "technologies": null,
    "description": "Interactive projection, real-time audio-reactive visuals, Développement d'une projection interactive",
    "impact": null,
    "impact-title": "SHADOW TUNER — Tuning global radio with human silhouettes in real time",
    "media": [
      "https://www.instagram.com/lozanohemmer/"
    ],
    "instagram": [
      "https://www.instagram.com/lozanohemmer/"
    ],
    "cover": null,
    "hasMedia": true,
    "fullDescription": "Shadow Tuner is an interactive installation that turns visitors’ shadows into a global tuning device. An animated image of the Earth rotates on a sphere, and as shadows fall across different cities, live local radio stations from those places are automatically triggered, creating a constantly shifting polyphonic soundscape.\nThe piece exists in two versions. The monumental version uses a custom-made spherical balloon with large-scale projection mapping: shadows cast by passers-by are tracked and appear at heights between 2 and 14 meters, depending on their distance from the camera. In the smaller version, the Earth rotates on a tight spherical LED display, with shadows mapped onto its surface as visitors move around it. In both cases, shadows covering cities activate their corresponding radio stations, blending voices and music from across the world into a single, shared instrument.\n**Technical details and implementation**\n- Infrared capture system provides reliable silhouette detection under variable lighting conditions\n- Advanced projection mapping renders edge-tracked forms with audio-reactive fills and textures\n- Live sound synthesis engine maps gesture scale, velocity, and dwell time to musical parameters\n- Multi-projector setup with sophisticated warping and blending for seamless large-surface coverage\n**Challenges and solutions**\n- Detecting shadows in mixed lighting required IR isolation and sophisticated background subtraction algorithms\n- Eliminating perceptual lag demanded optimized GPU pipelines and ultra-low-latency audio processing\n- Large-surface projection mapping required multi-projector warping, blending, and geometric calibration\n**Impact and results**\n- Encourages playful discovery and experimentation across all age groups\n- Creates profound poetic inversion—transforming absence into presence, darkness into expression\n- Demonstrates how technology can make the invisible visible and the negative positive\n**Process and methodology**\n- Extensive algorithm prototyping for contour detection and edge stability across various lighting conditions\n- Participatory testing with diverse user groups to balance responsiveness with visual clarity\n- Final site calibration including projection geometry optimization and sound pressure level tuning\n---",
    "videoEmbed": "https://vimeo.com/antimodular"
  },
  {
    "id": "collider",
    "title": "COLLIDER",
    "year": null,
    "client": "Rafael Lozano-Hemmer / Lulu Island",
    "role": null,
    "technologies": null,
    "description": "Real-time data visualization, cosmic radiation detection, Développement d'une visualisation en temps réel",
    "impact": null,
    "impact-title": "COLLIDER — Translating cosmic rays into kinetic curtains of light",
    "media": [
      "https://www.instagram.com/lozanohemmer/"
    ],
    "instagram": [
      "https://www.instagram.com/lozanohemmer/"
    ],
    "cover": null,
    "hasMedia": true,
    "fullDescription": "Collider is a light installation that translates invisible cosmic radiation into visible form. In its first iteration, presented at Translation Island in Abu Dhabi in 2023, hundreds of pencil-beam robotic searchlights formed a glimmering curtain of light visible from Lulu Island and downtown Abu Dhabi. The lights responded in real time to muons—subatomic particles produced when cosmic rays from stars and black holes collide with Earth’s atmosphere. Sensors adapted from Geiger counters detected the angle of these particles, and the live data was mapped directly into the shifting geometry of the light curtain.\nThe second iteration, installed on the façade of the Parrish Art Museum’s Herzog & de Meuron building, replaced searchlights with hundreds of small LED spotlights. Here, the data shaped a calm, rippling curtain of light across the museum’s architecture, offering a more intimate encounter with the same cosmic phenomena. In both versions, Collider turns distant astrophysical events into a perceptible, architectural experience.\n**Technical details and implementation**\n- Live data feed from cosmic ray detectors captures real-time radiation events\n- Custom visualization engine maps event energy and angle to dynamic particles, trails, and bursts\n- High-resolution projection or display systems ensure clarity under ambient lighting conditions\n- Sophisticated data processing handles the stochastic nature of cosmic ray detection\n**Challenges and solutions**\n- Managing data dropouts and noise required robust buffering and advanced denoising strategies\n- Balancing scientific legibility with visual awe demanded a visual grammar that scales from micro to macro events\n- Creating engaging visualization from inherently random data required careful design of visual metaphors\n**Impact and results**\n- Connects everyday spaces with cosmic phenomena, fostering scientific curiosity across all ages\n- Successfully bridges science outreach and media art, making complex physics accessible and beautiful\n- Demonstrates how real-time data visualization can transform abstract science into visceral experience\n**Process and methodology**\n- Collaboration with physicists and research institutions for authentic sensor access and data validation\n- Intensive design sprints to develop visual metaphors and language for cosmic phenomena\n- In-situ tuning and calibration for optimal viewing under various ambient lighting conditions\n---",
    "videoEmbed": "https://vimeo.com/1020229057",
    "videoEmbeds": ["https://vimeo.com/1020229057", "https://vimeo.com/904519362"]
  },
  {
    "id": "recurrent-first-dream",
    "title": "RECURRENT FIRST DREAM",
    "year": null,
    "client": "Rafael Lozano-Hemmer / ZONAMACO ARTFAIR",
    "role": null,
    "technologies": null,
    "description": "Oeuvre d'art générative",
    "impact": null,
    "impact-title": "RECURRENT FIRST DREAM — Rendering Sor Juana's poetry as a generative, slow-motion flame",
    "media": [
      "https://www.instagram.com/lozanohemmer/"
    ],
    "instagram": [
      "https://www.instagram.com/lozanohemmer/"
    ],
    "cover": null,
    "hasMedia": true,
    "fullDescription": "Recurrent First Dream (Primero Sueño Recurrente) is an algorithmic animation based on Sor Juana Inés de la Cruz’s 1692 poem Primero Sueño, a feminist ode to transcendence, knowledge, and deductive reasoning. Created during a period when Sor Juana faced censorship from the Catholic Church and the Inquisition, the poem reflects her struggle to uphold rationalist principles while living within a religious order.\nThe installation employs a double layer of language: beneath lies the full text of the poem, while above it golden words drift like streamers, animated through fluid dynamics to resemble a flame in slow motion—the flame of knowledge. In its standalone version, the work unfolds on an elongated screen, where an ascending vortex of words gradually reveals the transcendental scope of Sor Juana’s vision.\n**Technical details and implementation**\n- Custom generative engine drives multi-screen sequences that never repeat, creating infinite variations\n- Textual structures from the original poem inform graphical grammars, pacing, and visual composition\n- Four-screen synchronization system with drift-resistant clocking ensures perfect temporal alignment\n- Advanced shader optimization and frame budgeting manage GPU load across multiple displays\n**Challenges and solutions**\n- Avoiding perceptual repetition required massive state spaces and sophisticated stochastic pathing algorithms\n- Managing GPU load across four screens demanded shader optimization and careful frame budgeting\n- Creating meaningful visual poetry required deep understanding of both literary structure and generative systems\n**Impact and results**\n- Successfully bridges literature and code, making viewers sense an infinite poem made visible\n- Demonstrates versatility by working effectively in both art fair and gallery contexts\n- Creates meditative, contemplative experience that rewards extended viewing\n**Process and methodology**\n- Extensive poetic research with motif and semantic mapping to understand the original text's structure\n- Parameter studies to balance legibility with abstraction, ensuring the work remains accessible\n- Rigorous rack-level testing for multi-screen synchronization and technical reliability\n---",
    "videoEmbed": "https://vimeo.com/957965753"
  },
  {
    "id": "translation-island",
    "title": "TRANSLATION ISLAND",
    "year": null,
    "client": "Rafael Lozano-Hemmer / ABU DHABI",
    "role": null,
    "technologies": null,
    "description": "Parcours d'art en extérieur",
    "impact": null,
    "impact-title": "TRANSLATION ISLAND — Building a 2-km interactive trail with AI, sensors, and projection",
    "media": [
      "https://www.instagram.com/lozanohemmer/"
    ],
    "instagram": [
      "https://www.instagram.com/lozanohemmer/"
    ],
    "cover": null,
    "hasMedia": true,
    "fullDescription": "Translation Island was a large-scale outdoor exhibition trail on Lulu Island in Abu Dhabi, presented from November 20, 2023, to January 31, 2024, as part of the inaugural MANAR Light Festival. Ten audiovisual artworks—six of them world premieres—transformed the deserted island into a two-kilometre-long night-time path through sand dunes, desert flora, beaches, and freshwater lakes.\nCommissioned by Abu Dhabi’s Department for Culture and Tourism, the project was conceived as an “ode to translation,” understood both as the rendering of one language into another and as the relocation of subjects in space. The works combined AI, computer vision, environmental sensing, and 3D mapping to create poetic experiences in the desert landscape.\nPieces such as Collider translated cosmic radiation into a colossal curtain of light visible from 10 kilometers away, while Thermal Drift, Dune Ringers, and Pulse Island revealed biometric signatures—body heat, voice, heartbeat—through projection mapping and fields of glowing bulbs. Other works addressed language directly: Translation Stream projected streams of letters that reorganized around visitors to reveal poems by Emirati poets in Arabic and English; Shadow Tuner allowed visitors to tune into thousands of live radio stations worldwide with their shadows; and Translation Lake used AI voice avatars to endlessly translate James Joyce’s Finnegan’s Wake into 24 languages.\nBy gathering these pieces into a single nocturnal journey, Translation Island wove together histories of translation—from the Tower of Babel to the Library of Alexandria and the Abbasid House of Wisdom—while engaging with the promises and absurdities of contemporary AI.\nTechnical details and implementation\n- Weatherproof projection/lighting integrated with site features.\n- Sensors along the path to pace narrative reveals.\n- Multilingual text and audio assets for inclusive access.\nChallenges and solutions\n- Harsh outdoor conditions → sealed housings, high‑lumen projectors, scheduled maintenance.\n- Crowd flow → distributed triggers and clear path sightlines.\nImpact and results\n- Turns public space into a participatory story; invites repeat walks to catch different timings.\n- Signals the city’s commitment to cultural programming.\nProcess and methodology\n- Site survey and storyboarding per waypoint.\n- Modular rigging and power planning.\n- Night‑time calibration runs in situ.\n---",
    "videoEmbed": "https://vimeo.com/1035163031"
  },
  {
    "id": "atmospheric-memory",
    "title": "ATMOSPHERIC MEMORY",
    "year": null,
    "client": "Rafael Lozano-Hemmer / Sydney",
    "role": null,
    "technologies": null,
    "description": "Exposition immersive",
    "impact": null,
    "impact-title": "ATMOSPHERIC MEMORY — Converting voice and breath into immersive, volumetric media architectures",
    "media": [
      "https://www.instagram.com/ey_global/"
    ],
    "instagram": [
      "https://www.instagram.com/ey_global/"
    ],
    "cover": null,
    "hasMedia": true,
    "fullDescription": "Atmospheric Memory is a touring exhibition inspired by Charles Babbage’s 19th-century speculation that the air retains every word ever spoken. Spread across multiple rooms, the project uses voice-activated projections, robotics, and sound to turn atmosphere into both archive and instrument. Visitors speak into the installation and see their words transformed into text, light, and kinetic movement, creating an environment where memory and presence are made tangible.\nAt its core is The Chamber, a monumental room of immersive projection mapping powered by around 20 projectors, enveloping visitors in shifting layers of text and imagery. Other sections integrate thousands of single-channel speakers from Atmosphonia, building dense sonic ecosystems that respond to presence. Different moments of the exhibition are interlinked, so that visitors can encounter echoes of their earlier actions and see how their presence reverberates across spaces.\nDesigned for touring, the work uses modular rooms adaptable to different venues and architectural conditions. Complex systems integration across projections, robotics, and multichannel audio demanded robust show control and standardized installation methods. By linking historical computing concepts to contemporary questions of surveillance, privacy, and permanence, Atmospheric Memory offered audiences a large-scale, immersive reflection on how technology preserves—and reshapes—the traces we leave behind.\n**Technical details and implementation**\n- Multiple interconnected rooms combining projection, robotics, multi-channel sound, and interactive voice capture\n- Voice input drives complex text, visual, and kinetic responses across different media systems\n- Sophisticated touring adaptations accommodate varied venue sizes and architectural constraints\n- Modular room design allows flexible configuration for different exhibition contexts\n**Challenges and solutions**\n- Complex systems integration across multiple media types required modular room design and robust show control\n- Ensuring accessibility and clear onboarding demanded intuitive affordances and comprehensive staff training\n- Managing touring logistics required standardized installation procedures and efficient strike protocols\n**Impact and results**\n- Connects historical computing concepts to contemporary anxieties around memory, privacy, and surveillance\n- Generated significant public engagement and critical discourse in art and technology communities\n- Successfully toured multiple venues, demonstrating the work's adaptability and lasting relevance\n**Process and methodology**\n- Extensive historical research into Babbage's work and its contemporary implications\n- Room-by-room prototyping and playtesting to optimize user experience and technical reliability\n- Development of comprehensive touring playbooks for efficient installation and strike procedures\n---",
    "videoEmbed": "https://vimeo.com/1080206951"
  },
  {
    "id": "sync",
    "title": "SYNC",
    "year": null,
    "client": "Rafael Lozano-Hemmer / SAN FRANCISCO",
    "role": null,
    "technologies": null,
    "description": "Performance audio réactive",
    "impact": null,
    "impact-title": "SYNC — Mapping live drumming to a choreography of 3,000 bulbs across Pulse Topology",
    "media": [
      "https://www.instagram.com/lozanohemmer/"
    ],
    "instagram": [
      "https://www.instagram.com/lozanohemmer/"
    ],
    "cover": null,
    "hasMedia": true,
    "fullDescription": "Sync is a live performance collaboration with percussionist Eli Keszler where every strike, scrape, and resonance of the drum kit becomes light and image. Using Sensory Percussion to control an array of 3,000 light bulbs, each gesture expands into synchronized patterns with sub-frame precision. Built on Pulse Topology as its artistic infrastructure, the work reframes percussion as a multi-sensory practice, immersing audiences in sound made visible. Sync was presented at Pace Gallery in New York and at Gray Area in San Francisco.\nTechnical details and implementation\n- Sensory Percussion mapped to OSC/MIDI controlling projection and lighting.\n- TouchDesigner/real‑time pipeline tuned for sub‑frame responsiveness.\n- Adaptations for white‑cube (Pace) and stage contexts (San Francisco).\nChallenges and solutions\n- Capturing micro‑gestures without visual overload → scaled mappings and soft thresholds.\n- Latency management → direct device routing and GPU prioritization.\n- Rapid venue re‑calibration between radically different spaces.\nImpact and results\n- Reframed percussion as multi‑sensory practice; strong audience immersion.\n- Documented templates reused in later performance projects.\nProcess and methodology\n- Mapping rehearsals with performer to ‘play’ the visuals.\n- Preset banks for different pieces; live tweak panel for operator.\n- Tear‑down/bring‑up checklists per venue.\n---",
    "videoEmbed": null
  },
  {
    "id": "a-generative-movement",
    "title": "A GENERATIVE MOVEMENT",
    "year": null,
    "client": "Rafael Lozano-Hemmer & bitforms gallery / SAN FRANCISCO",
    "role": null,
    "technologies": null,
    "description": "Exposition solo",
    "impact": null,
    "impact-title": "OPHY BU GAINS / IA GÉNERATIVE — Generative artistic movements",
    "media": [
      "https://www.instagram.com/bitforms/"
    ],
    "instagram": [
      "https://www.instagram.com/bitforms/"
    ],
    "cover": null,
    "hasMedia": false,
    "videoEmbed": null
  },
  {
    "id": "techs-mechs",
    "title": "TECHS MECHS",
    "year": null,
    "client": "Rafael Lozano-Hemmer & Gray Area / SAN FRANCISCO",
    "role": null,
    "technologies": null,
    "description": "Exposition solo",
    "impact": null,
    "impact-title": "TECHS MECHS — Procedural robotics and AI driving performative light and motion systems",
    "media": [
      "https://www.instagram.com/grayareaorg/"
    ],
    "instagram": [
      "https://www.instagram.com/grayareaorg/"
    ],
    "cover": null,
    "hasMedia": true,
    "fullDescription": "Techs Mechs was a solo exhibition at Gray Area in San Francisco showcasing three works: Airborne Newscast, a dual-projector shadow-tracking piece; a four-screen version of Recurrent First Dream; and an experimental Thermal Drift Density Map. Together, they demonstrated the breadth of realtime techniques in the studio, from tracking and generative poetics to thermal imaging.\nThe show required careful GPU tuning to keep multi-screen systems stable and precise calibration to manage dual-projector blending and shadow occlusion. The Thermal Drift experiment also pushed new rendering approaches for thermal sensing under gallery conditions.\nBeyond presenting individual works, Techs Mechs consolidated a coherent exhibition language that highlighted the interplay between custom tracking, generative systems, and media-poetic experimentation.\nTechnical details and implementation\n- Airborne Newscast: two mapped projectors; live shadow tracking and projection.\n- Recurrent First Dream: synced 4‑screen generative choreography.\n- Thermal Drift Density Map: heat‑field into density visualization; new render path.\nChallenges and solutions\n- Dual projector blending and shadow occlusion management.\n- GPU tuning for multi‑screen stability.\n- Thermal denoising and stability under gallery traffic.\nImpact and results\n- Showed breadth from tracking to generative poetics; strong institutional interest.\n- Laid technical groundwork for subsequent installations.\nProcess and methodology\n- Piece‑by‑piece prototyping; common show control backbone.\n- On‑site alignment and visitor‑flow tuning.\n- Post‑show documentation for redeployment.\n---",
    "videoEmbed": "https://www.youtube.com/watch?v=SmJPJVzUcCo"
  },
  {
    "id": "biometric-theatre",
    "title": "BIOMETRIC THEATRE",
    "year": null,
    "client": "Rafael Lozano-Hemmer / HONG KONG",
    "role": null,
    "technologies": null,
    "description": "Exposition solo",
    "impact": null,
    "impact-title": "BIOMETRIC THEATRE — Staging pulse, voice, and movement as audience-driven scenography",
    "media": [
      "https://www.instagram.com/lozanohemmer/"
    ],
    "instagram": [
      "https://www.instagram.com/lozanohemmer/"
    ],
    "cover": null,
    "hasMedia": true,
    "fullDescription": "Biometric Theatre was an immersive environment in Hong Kong where collective physiology shaped large-scale projections. Sensors measured heartbeat, respiration, and body temperature, transforming the audience’s inner rhythms into shifting audiovisual compositions. A monumental Thermal Drift visualization mapped the group’s heat signatures, while Hormonium rendered cyclical biological patterns at architectural scale.\nScaling biometric input to a crowd required anonymized, aggregate mappings that balanced clarity with privacy. The projections were calibrated for variable ambient light, ensuring visibility without overwhelming the site. By making inner states visible and shareable, Biometric Theatre turned personal data into a collective performance, sparking public curiosity and debate around the ethics of sensing technologies in art.\nTechnical details and implementation\n- Heartbeat/respiration/thermal sensing; anonymized, aggregate mappings.\n- Large Thermal Drift projection; large Hormonium projection.\n- Sound design tied to biometric tempo envelopes.\nChallenges and solutions\n- Scaling biometric input to crowd contexts without noise or privacy issues.\n- Architectural projection clarity under variable ambient light.\nImpact and results\n- Made inner states shareable; strong public curiosity and participation.\n- Advanced discourse around data ethics in art contexts.\nProcess and methodology\n- Sensor calibration for groups; consent/onboarding UI.\n- Mapping workshops to keep outputs legible and meaningful.\n- Live ops playbook to maintain stability through peak hours.\n---",
    "videoEmbed": null
  },
  {
    "id": "thermal-drift",
    "title": "THERMAL DRIFT",
    "year": null,
    "client": "Rafael Lozano-Hemmer / NYC ARMORY ARTFAIR",
    "role": null,
    "technologies": null,
    "description": "Oeuvre d'art interactive",
    "impact": null,
    "impact-title": "THERMAL DRIFT — Transforming body heat into evolving, audio-reactive landscapes",
    "media": [
      "https://www.instagram.com/lozanohemmer/"
    ],
    "instagram": [
      "https://www.instagram.com/lozanohemmer/"
    ],
    "cover": null,
    "hasMedia": true,
    "fullDescription": "Thermal Drift transforms human heat into visible poetry. A thermal camera captures bodies, turning their energy into drifting particles that swirl, dissolve, and flow across the screen. Over a six-minute narrative arc, the work shifts from pure abstraction to figuration, guided by an audio-reactive soundtrack that drives its density and rhythm. Adaptive calibration ensures clarity in any environment, allowing the piece to resonate whether projected in vast venues or intimate settings. Exhibited worldwide—including the NYC Armory, Basel 2022, and Crystal Bridges—Thermal Drift has become a signature work collected by both private and institutional collections, a testament to the poetic potential of thermal imaging.\n**Technical details and implementation**\n- High-sensitivity thermal camera with sophisticated calibrated background profiling\n- Custom particle shader system with audio-reactive modulation controlling density and flow patterns\n- Comprehensive editioning pipeline optimized for gallery and art fair deployment\n- Advanced background subtraction algorithms maintain visual contrast across diverse installation sites\n**Challenges and solutions**\n- Maintaining visual contrast across different sites required adaptive background subtraction and environmental calibration\n- Handling high visitor throughput demanded particle pooling, culling algorithms, and dynamic resolution scaling\n- Creating consistent experience across venues required flexible adaptation for various screen and projection setups\n**Impact and results**\n- Exhibited at prestigious venues including NYC Armory Art Fair, Basel 2022, and Crystal Bridges' Listening Forest\n- Multiple editions sold to private and institutional collections worldwide\n- Established as a signature work that demonstrates the poetic potential of thermal imaging technology\n**Process and methodology**\n- Iterative prototyping with various thermal sensors and lens configurations to optimize image quality\n- Careful narrative curve design that guides viewers from abstraction to figuration over six minutes\n- Per-site adaptation protocols for different display technologies and visitor traffic patterns\n---",
    "videoEmbed": "https://vimeo.com/923878663"
  },
  {
    "id": "listening-forest",
    "title": "LISTENING FOREST",
    "year": null,
    "client": "Rafael Lozano-Hemmer / CRYSTAL BRIDGES MUSEUM - ARKAMSAS",
    "role": null,
    "technologies": null,
    "description": "Parcours d'art en extérieur",
    "impact": null,
    "impact-title": "LISTENING FOREST — Distributing sensors across a night trail to localize sound and light responses",
    "media": [
      "https://www.instagram.com/crystalbridgesmuseum/"
    ],
    "instagram": [
      "https://www.instagram.com/crystalbridgesmuseum/"
    ],
    "cover": null,
    "hasMedia": true,
    "fullDescription": "Listening Forest was a night-time exhibition trail at Crystal Bridges in Arkansas, where distributed interactive artworks transformed the woods into a responsive landscape. Spread along a long outdoor path, the works included Arkansas Text Stream, an outdoor adaptation of Thermal Drift, and Summon, a collective response node that gathered visitors’ presence.\nThe challenge of weatherproofing and running reliable power and data across distance shaped the design, while careful planning ensured safety and wayfinding without breaking the mystery of the trail. Calibration and maintenance cycles were carried out at night, and visitor flow was tested to avoid bottlenecks.\nAs the first project of its kind for Rafael Lozano-Hemmer’s studio, Listening Forest set a precedent for large-scale outdoor exhibitions, paving the way for later projects such as Translation Island. By blending projection, light, and sensing into the forest environment, it expanded the museum’s audience and offered families and communities a shared encounter with media art in nature.\nTechnical details and implementation\n- Distributed projection/lights and sensing along a long outdoor path.\n- Arkansas Text Stream; Thermal Drift (outdoor); Summon as collective response node.\nChallenges and solutions\n- Weatherproofing and reliable power/data runs over distance.\n- Designing for wayfinding and safety while preserving mystery.\nImpact and results\n- Major public draw; broadened the museum’s audience profile.\n- Model for blending landscape and media art.\nProcess and methodology\n- Trail survey and node planning; cable/power routing.\n- Outdoor calibration at night; maintenance cycles.\n- Visitor‑flow tests to avoid bottlenecks.\n---",
    "videoEmbed": "https://www.youtube.com/watch?v=9iu9Vkh6Oos"
  },
  {
    "id": "excuse-you",
    "title": "EXCUSE YOU",
    "year": null,
    "client": "Rafael Lozano-Hemmer & Wilde Gallery / BASEL",
    "role": null,
    "technologies": null,
    "description": "Exposition solo",
    "impact": null,
    "impact-title": "EXCUSE YOU — Curating multi-work media into a cohesive, gallery-grade system",
    "media": [
      "https://www.instagram.com/lozanohemmer/"
    ],
    "instagram": [
      "https://www.instagram.com/lozanohemmer/"
    ],
    "cover": null,
    "hasMedia": true,
    "fullDescription": "Excuse You was a solo exhibition at Wilde Gallery in Basel that brought together several works into a unified installation. The presentation included a Thermal Drift Density Map, a projection of Hormonium, and an adapted Parametric Staircase. Rather than treating each piece separately, the exhibition emphasized coherence, with each work carefully installed to meet the expectations of a gallery setting.\nIn this context, technical experiments had to be presented as polished, finished artworks—framed, calibrated, and refined to feel at home in the white-cube environment. By consolidating these pieces into a cohesive presentation, Excuse You strengthened collector interest and demonstrated the studio’s ability to translate research-driven installations into a gallery-ready form.\nTechnical details and implementation\n- Gallery‑scale Thermal Drift Density Map variant.\n- Hormonium projection install; Parametric Staircase adaptation.\n- Unified show control and monitoring.\nChallenges and solutions\n- Time‑boxed installation with diverse technical needs.\n- Maintaining coherence across heterogeneous works.\nImpact and results\n- Strengthened gallery relationships and collector interest.\n- Demonstrated production capabilities for complex multi‑work shows.\nProcess and methodology\n- Advance planning with detailed floor plan and IO map.\n- On‑site adjustments for scale and viewing distances.\n- Post‑install testing under opening‑night conditions.\n---",
    "videoEmbed": "https://www.youtube.com/watch?v=dQ3dSjyJHrg"
  },
  {
    "id": "haciendo-agua",
    "title": "HACIENDO AGUA",
    "year": null,
    "client": "Rafael Lozano-Hemmer & Max Estrella Gallery / MADRID",
    "role": null,
    "technologies": null,
    "description": "Exposition solo",
    "impact": null,
    "media": [
      "https://www.instagram.com/lozanohemmer/"
    ],
    "instagram": [
      "https://www.instagram.com/lozanohemmer/"
    ],
    "cover": null,
    "hasMedia": false,
    "videoEmbed": null
  },
  {
    "id": "caudales",
    "title": "CAUDALES",
    "year": null,
    "client": "Rafael Lozano-Hemmer / MADRID CASA DE MEXICO",
    "role": null,
    "technologies": null,
    "description": "Exposition d'oeuvres génératives et interactives",
    "impact": null,
    "impact-title": "CAUDALES — Curating algorithmic text works into a coherent exhibition grammar",
    "media": [
      "https://www.instagram.com/casademexicoenespana/"
    ],
    "instagram": [
      "https://www.instagram.com/casademexicoenespana/"
    ],
    "cover": null,
    "hasMedia": true,
    "fullDescription": "Caudales is a suite of generative text installations that transform literature, philosophy, and critical theory into flowing streams of words. Exhibited from February 25 to April 24, 2022 at Casa de México in Madrid, the show brought together eight works under a shared curatorial frame linking textuality and visual abstraction.\nUsing algorithms borrowed from fluid dynamics and non-linear systems, the pieces deconstruct and recompose texts by authors such as Sor Juana Inés de la Cruz, George Orwell, Julio Cortázar, and Stuart Hall. One standout is Primero Sueño Recurrente, rendered through 45 flat suspended screens forming a spiraling paraboloid; words emerge as an ascending vortex across the screens.\nRather than delivering literal readings, Caudales foregrounds unstable typographic flows and the agency of the viewer in navigating meaning.\nTechnical details and implementation\nDual‑projector tracking (Airborne Newscast); kinetic/light components (Chandelier).Installation of Encode/Decode and Mallarmé; Hormonium display.Challenges and solutions\nIntegrating technically diverse works into one dramaturgy.International install logistics and coordination.Impact and results\nCreated a dense, multi‑modal experience anchoring literary references.Expanded institutional network in Spain.Process and methodology\nPiece‑specific prebuilds; crate/ship documentation.On‑site sequencing and light levels per room.Handover docs for local technicians.---",
    "videoEmbed": null
  },
  {
    "id": "hormonium",
    "title": "HORMONIUM",
    "year": null,
    "client": "Rafael Lozano-Hemmer / MADRID",
    "role": null,
    "technologies": null,
    "description": "Oeuvre d'art générative",
    "impact": null,
    "impact-title": "HORMONIUM — Visualizing circadian, ultradian, and infradian cycles as airborne text waves",
    "media": [
      "https://www.instagram.com/lozanohemmer/"
    ],
    "instagram": [
      "https://www.instagram.com/lozanohemmer/"
    ],
    "cover": null,
    "hasMedia": true,
    "fullDescription": "Hormonium (Text Stream 8) is a generative artwork that visualizes human chronobiology through waves of text. Sequences of ocean waves crash across the screen, releasing airborne text particles that represent acronyms of hormones. Each hormone appears according to its natural rhythm, turning the invisible cycles of the body into flowing audiovisual patterns.\nThe piece incorporates circadian rhythms, releasing cortisol, progesterone, and testosterone in the morning; FSH and LH in the afternoon; and oestradiol and prolactin at night. Ultradian rhythms—shorter cycles within the day—are shown through leptin and ghrelin fluctuations. Infradian rhythms—longer cycles such as the 28-day hormonal variations of estrogen, FSH, progesterone, and LH—also appear. Finally, a 90-year lifespan cycle charts long-term changes such as the decline of aldosterone, calcitonin, GH, and renin, and the rise of cortisol. After 90 years, the work resets, beginning the cycles anew.\nBy mapping these rhythms into an endless ocean of words, Hormonium makes the body’s hidden cycles perceptible through the language of waves.\nTechnical details and implementation\n- Cyclical waveform engines with parameterized phase/period relationships.\n- Projection mapping or screen presentation; ambient or focal modes.\nChallenges and solutions\n- Keeping long‑run interest → slow evolution with punctuated ‘events’.\n- Scaling from intimate to architectural surfaces.\nImpact and results\n- Exhibited internationally and sold in multiple editions over several years.\n- Frequently requested due to its formal clarity and atmospheric depth.\nProcess and methodology\n- Ongoing code refinements and palette libraries.\n- Per‑venue pacing presets and brightness profiles.\n---",
    "videoEmbed": "https://vimeo.com/953968166"
  },
  {
    "id": "lcs-opening-ceremony",
    "title": "LCS OPENING CEREMONY",
    "year": null,
    "client": "Moment Factory / LOS ANGELES",
    "role": null,
    "technologies": null,
    "description": "Réalité augmentée en direct",
    "impact": null,
    "media": [
      "https://www.instagram.com/lolesports/"
    ],
    "instagram": [
      "https://www.instagram.com/lolesports/"
    ],
    "cover": null,
    "hasMedia": false,
    "videoEmbed": null
  },
  {
    "id": "ecosystemes",
    "title": "ECOSYSTEMES",
    "year": null,
    "client": "MAPP MTL - Aude Guivarch / MONTREAL",
    "role": null,
    "technologies": null,
    "description": "Mapping",
    "impact": null,
    "impact-title": "ÉCOSYSTEMES (Ecosystem Alpha) — Projection-mapped terrain reacting to proximity, touch, and regeneration",
    "media": [
      "https://www.instagram.com/audemaeva/"
    ],
    "instagram": [
      "https://www.instagram.com/audemaeva/"
    ],
    "cover": null,
    "hasMedia": true,
    "fullDescription": "Écosystemes (Ecosystem Alpha) is an interactive video-mapping sculpture created by Aude Guivarc’h, with interactivity programmed by Hugo Daoust. The work presents a fictional fragment of mountainous terrain as a “sample of planet Earth,” built from 3D-printed forms coated in plaster and precisely mapped with 4K projection, Kinect 2.0 depth sensing, and a stereo sound system.\nNatural cycles of water, wind, erosion, and the seasons animate the surface, bringing the static relief to life. When visitors approach, pass a hand in front of the projection, or touch the sculpture, the ecosystem reacts with fire and destruction. The more it is disturbed, the longer it takes to regenerate. This mechanic makes each visitor directly responsible for the health of the landscape and the experience of others.\nPremiered in 2021 at Never Apart Gallery in Montreal and permanently acquired by Age of Union’s Earth Center in 2022, Écosystemes is both poetic and provocative—raising awareness of our impact on fragile environments by turning every gesture into an ecological choice.\nNatural cycles of water, wind, erosion, and the seasons animate the surface, bringing the static relief to life. When visitors approach, pass a hand in front of the projection, or touch the sculpture, the ecosystem reacts with fire and destruction. The more it is disturbed, the longer it takes to regenerate. This mechanic makes each visitor directly responsible for the health of the landscape and the experience of others.\nPremiered in 2021 at Never Apart Gallery in Montreal and permanently acquired by Age of Union’s Earth Center in 2022, Écosystemes is both poetic and provocative—raising awareness of our impact on fragile environments by turning every gesture into an ecological choice.\nTechnical details and implementation\n- Large‑format projection mapping onto irregular facades.\n- Visual sequences composed as ecological cycles; adaptive brightness for urban light.\nChallenges and solutions\n- Facade irregularity and ambient light → detailed mesh scans and luminance compensation.\n- Maintaining legibility from varied street vantage points.\nImpact and results\n- Activated public space for broad audiences; approachable yet conceptually grounded.\n- Advanced the festival’s discourse on mapping as civic storytelling.\nProcess and methodology\n- On‑site scans and tests to author content to the surface.\n- Evening‑by‑evening tweaks as conditions changed.\n---",
    "videoEmbed": null
  },
  {
    "id": "animistic-imagery",
    "title": "ANIMISTIC IMAGERY",
    "year": null,
    "client": "Moment Factory / BEIJING",
    "role": null,
    "technologies": null,
    "description": "Déambulatoire interactif",
    "impact": null,
    "impact-title": "ANIMISTIC IMAGERY — Remote-produced AI studio translating visitor motion into generative art",
    "media": [
      "https://www.instagram.com/momentfactory/"
    ],
    "instagram": [
      "https://www.instagram.com/momentfactory/"
    ],
    "cover": null,
    "hasMedia": true,
    "fullDescription": "Animistic Imagery was an immersive AI art experience created by Moment Factory for UCCA Lab at the UCCA Center for Contemporary Art in Beijing. Developed in collaboration with Baidu, the project explored the relationship between humans, artificial intelligence, and art.\nAt the heart of the exhibition was Duffy, an AI “artist” who invited visitors to collaborate inside her Symbiotic Studio. Through real-time tracking, projection mapping, and interactive technology, guests became muses for the AI. Their movements were captured and transformed into an infinite series of images, as Duffy drew on vast datasets of colors and archetypal forms from the natural world. The result was an ongoing stream of surprising, artificial interpretations of humanity and nature.\nOpen from September 2020 to January 2021, the exhibition was Moment Factory’s first fully remote production. The Montreal studio designed and directed the experience while UCCA Lab and technical partner Digital Fun supported on-site integration. Despite the pandemic, the project successfully introduced Chinese audiences to a playful collaboration between people and machines.\nTechnical details and implementation\n- Multi‑room projections and soundscapes; motion‑driven narrative triggers.\n- Remote production pipelines; delivery of calibrated presets and layouts.\nChallenges and solutions\n- Reliability without onsite iteration → comprehensive simulation and redundancy.\n- Cultural sensitivity while abstracting motifs → advisory and review loops.\nImpact and results\n- Successful large‑scale deployment despite distance; strong public response.\n- Exemplar of remote‑first production methodology.\nProcess and methodology\n- Tight spec docs, screen maps, and IO diagrams.\n- Async QA with local integrators; contingency fallback states.\n---",
    "videoEmbed": "https://www.youtube.com/watch?v=FNbxkb9bf50"
  },
  {
    "id": "my-morning-jacket-album-launch",
    "title": "MY MORNING JACKET ALBUM LAUNCH",
    "year": null,
    "client": "Moment Factory / DIFFUSION WEB",
    "role": null,
    "technologies": null,
    "description": "Création de contenu",
    "impact": null,
    "media": [
      "https://www.instagram.com/mymorningjacket/"
    ],
    "instagram": [
      "https://www.instagram.com/mymorningjacket/"
    ],
    "cover": null,
    "hasMedia": false,
    "videoEmbed": null
  },
  {
    "id": "billie-eilish-2020-world-tour",
    "title": "BILLIE EILISH 2020 WORLD TOUR",
    "year": null,
    "client": "Moment Factory / WORLD",
    "role": null,
    "technologies": null,
    "description": "Création de contenu temps réel",
    "impact": null,
    "impact-title": "BILLIE EILISH — 2020 WORLD TOUR — Notch + disguise pipeline delivering adaptive arena visuals",
    "media": [
      "https://www.instagram.com/billieeilish/"
    ],
    "instagram": [
      "https://www.instagram.com/billieeilish/"
    ],
    "cover": null,
    "hasMedia": true,
    "fullDescription": "Billie Eilish – 2020 World Tour featured large-scale visuals tightly synchronized to the artist’s performance. As part of Moment Factory’s creative team, I contributed to the design and programming of interactive and video content that adapted across arenas worldwide. The show combined bold graphic sequences with immersive effects that responded directly to Eilish’s stage presence, reinforcing her intimate yet cinematic style.\nCritics consistently highlighted the show’s immersive visuals and tight music–image sync. Rolling Stone’s coverage around the 2020 run and the Where Do We Go? livestream praised the arena/livestream productions for “moody, immersive visuals” that elevated the performance, while Billboard’s recap called the livestream “one-of-a-kind,” noting how each song’s digital environment amplified Eilish’s presence. Trade outlets documented the behind-the-scenes toolchain—Notch driving real-time looks and disguise handling playback—linking the aesthetic polish fans noticed to a robust real-time pipeline. The tour itself kicked off March 10, 2020, before pandemic shutdowns, with later virtual editions reinforcing the same design ethos that reviewers singled out.\nThe production made extensive use of Notch for real-time visuals and disguise media servers for content distribution. We worked in close collaboration with the video and lighting teams to ensure seamless integration across departments. Live chroma keying techniques were also used on stage, allowing performers to be composited directly into the visuals during the show.\nTechnical details and implementation\n- Notch real‑time effects pipeline; Disguise media servers.\n- Timecode and live operator control for adaptive moments.\n- Integration with lighting cues and camera feeds.\nChallenges and solutions\n- Consistency across venues with different LED inventories → modular scene graphs.\n- Keeping energy aligned with performance while avoiding visual fatigue.\nImpact and results\n- Delivered a cohesive high‑impact visual identity for a global pop tour.\n- Showcased robustness of real‑time pipelines in stadium contexts.\nProcess and methodology\n- Rehearsal‑driven mapping from musical structure to visual behavior.\n- Operator playbooks for show‑by‑show tweaks.\n---",
    "videoEmbed": "https://www.youtube.com/playlist?list=PLDl5MyZwy_wKYjJPj1i1DCTYebBP1qlGU"
  },
  {
    "id": "panasonic-augmented-basketball-court",
    "title": "PANASONIC AUGMENTED BASKETBALL COURT",
    "year": null,
    "client": "Moment Factory / MONTRÉAL",
    "role": null,
    "technologies": null,
    "description": "R&D",
    "impact": null,
    "impact-title": "PANASONIC AUGMENTED BASKETBALL COURT — Ultra-low-latency tracking and projection for responsive gameplay",
    "media": [
      "https://www.instagram.com/momentfactory/"
    ],
    "instagram": [
      "https://www.instagram.com/momentfactory/"
    ],
    "cover": null,
    "hasMedia": true,
    "fullDescription": "High-Speed Tracking: Basketball Court was an R&D project that merged multiple sensing systems—3D LiDARs, infrared cameras, and high-speed tracking—into a real-time projection mapping setup. Graphics were overlaid directly onto gameplay, with the court and walls reacting instantly to dribbles, collisions, and ball movement. This fusion of technologies created a uniquely responsive environment where athletic gestures became dynamic visual events. The prototype not only showcased the artistic potential of projection in sports but also proved its practical value, leading directly to an NBA contract.\n**Technical details and implementation**\n- High-speed projection system with ultra-low-latency tracking of players and ball movement\n- Sophisticated camera/projector calibration system ensures perfect alignment with court geometry\n- Robust anti-glare strategies and high-lumen projection for visibility under intense gym lighting\n- Real-time gameplay overlays that respond to player actions and game dynamics\n**Challenges and solutions**\n- Achieving ultra-low latency required optimized tracking pipelines and predictive algorithms\n- Maintaining visibility under harsh gym lighting demanded high-lumen projection and careful surface preparation\n- Creating reliable tracking in dynamic sports environment required advanced computer vision and machine learning\n**Impact and results**\n- Prototype successfully proved value to professional sports partner, leading to NBA contract\n- Opened new pathways for augmented reality in training and fan experiences\n- Demonstrated how experimental media art can evolve into practical commercial applications\n**Process and methodology**\n- Closed-loop testing with professional athletes to optimize tracking accuracy and overlay timing\n- Iterative development of overlay rules and visual feedback systems\n- Comprehensive resiliency testing for sweat, dust, and frequent play conditions\n---",
    "videoEmbed": null
  },
  {
    "id": "ocean-park-illumination",
    "title": "OCEAN PARK ILLUMINATION",
    "year": null,
    "client": "Moment Factory / HONG KONG",
    "role": null,
    "technologies": null,
    "description": "Installation interactive",
    "impact": null,
    "media": [
      "https://www.instagram.com/hkoceanpark/"
    ],
    "instagram": [
      "https://www.instagram.com/hkoceanpark/"
    ],
    "cover": null,
    "hasMedia": false,
    "videoEmbed": null
  },
  {
    "id": "avett-brothers-tour",
    "title": "AVETT BROTHERS TOUR",
    "year": null,
    "client": "Moment Factory /",
    "role": null,
    "technologies": null,
    "description": "Création de contenu",
    "impact": null,
    "impact-title": "AVETT BROTHERS TOUR — Tour-ready hybrid of precomp and real-time looks tuned per venue",
    "media": [
      "https://www.instagram.com/theavettbrothers/"
    ],
    "instagram": [
      "https://www.instagram.com/theavettbrothers/"
    ],
    "cover": null,
    "hasMedia": true,
    "fullDescription": "Avett Brothers Tour featured a responsive visual system that amplified the band’s folk-rock performances with real-time imagery. Built in TouchDesigner, the setup mixed pre-rendered motion design with generative elements that reacted to audio and lighting cues. The aim was to create visuals that felt hand-crafted and warm, in tune with the band’s acoustic textures, while still delivering arena-scale impact.\nThe system was designed to be lightweight and tour-ready, allowing for quick adaptation across venues. By blending subtle interactivity with a strong design language, the visuals supported the Avett Brothers’ stage presence without overwhelming it, adding atmosphere and narrative accents to the live show.\nTechnical details and implementation\n- LED wall content plus live camera treatments.\n- Timing strategies for variable setlists; per‑song looks.\nChallenges and solutions\n- Venue variability → scalable layouts and per‑load‑in checks.\n- Maintaining emotional tone alongside storytelling songs.\nImpact and results\n- Enhanced audience immersion while preserving focus on performers.\n- Demonstrated tour‑ready reliability of the media package.\nProcess and methodology\n- Rehearsal capture -> look development -> show files.\n- Operator notes per venue; quick‑switch scene banks.\n---",
    "videoEmbed": null
  },
  {
    "id": "halsey-world-tour",
    "title": "HALSEY WORLD TOUR",
    "year": null,
    "client": "Moment Factory / LOS ANGELES",
    "role": null,
    "technologies": null,
    "description": "Création de contenu",
    "impact": null,
    "media": [
      "https://www.instagram.com/iamhalsey/"
    ],
    "instagram": [
      "https://www.instagram.com/iamhalsey/"
    ],
    "cover": null,
    "hasMedia": false,
    "videoEmbed": null
  },
  {
    "id": "breathless-london-art-now",
    "title": "BREATHLESS : LONDON ART NOW",
    "year": null,
    "client": "Ca' Pesaro Galleria Internazionale d'Arte Moderna & Ed Fornieles / VENEZIA",
    "role": null,
    "technologies": null,
    "description": "Programmation Unity et intégration",
    "impact": null,
    "media": [],
    "instagram": [],
    "cover": null,
    "hasMedia": false,
    "videoEmbed": null
  },
  {
    "id": "hines-louisiana",
    "title": "HINES LOUISIANA",
    "year": null,
    "client": "Float4 / MONTRÉAL-HOUSTON",
    "role": null,
    "technologies": null,
    "description": "Contenu génératif",
    "impact": null,
    "media": [],
    "instagram": [],
    "cover": null,
    "hasMedia": false,
    "videoEmbed": null
  },
  {
    "id": "rock-in-rio",
    "title": "ROCK IN RIO",
    "year": null,
    "client": "Moment Factory / MONTRÉAL-RIO",
    "role": null,
    "technologies": null,
    "description": "Mapping + Tracking en temps réel",
    "impact": null,
    "media": [
      "https://www.instagram.com/momentfactory/"
    ],
    "instagram": [
      "https://www.instagram.com/momentfactory/"
    ],
    "cover": null,
    "hasMedia": false,
    "videoEmbed": null
  },
  {
    "id": "halte-00-îles-de-boucherville",
    "title": "HALTE 00 ÎLES-DE-BOUCHERVILLE",
    "year": null,
    "client": "SÉPAQ / MONTRÉAL",
    "role": null,
    "technologies": null,
    "description": "Architecture + Interactivité",
    "impact": null,
    "media": [],
    "instagram": [],
    "cover": null,
    "hasMedia": false,
    "videoEmbed": null
  },
  {
    "id": "d-vernissage-calder",
    "title": "D-VERNISSAGE CALDER",
    "year": null,
    "client": "Musée des Beaux-Arts / MONTRÉAL",
    "role": null,
    "technologies": null,
    "description": "Installation laser/lumière",
    "impact": null,
    "media": [],
    "instagram": [],
    "cover": null,
    "hasMedia": false,
    "videoEmbed": null
  },
  {
    "id": "le-choeur-de-géants",
    "title": "LE CHOEUR DE GÉANTS",
    "year": null,
    "client": "Les Univers Givrés / SHAWINIGAN",
    "role": null,
    "technologies": null,
    "description": "Installation interactive",
    "impact": null,
    "media": [],
    "instagram": [],
    "cover": null,
    "hasMedia": false,
    "videoEmbed": null
  },
  {
    "id": "dead-obies-doo-wop",
    "title": "DEAD OBIES DOO WOP",
    "year": null,
    "client": "Telescope Production / MONTRÉAL",
    "role": null,
    "technologies": null,
    "description": "Performance laser",
    "impact": null,
    "media": [
      "https://www.instagram.com/deadobies/"
    ],
    "instagram": [
      "https://www.instagram.com/deadobies/"
    ],
    "cover": null,
    "hasMedia": true,
    "fullDescription": "Dead Obies – Doo Wop was a music video for the Montreal hip hop group. I created laser animations in TouchDesigner that were programmed directly for camera—no rehearsals, just precise cueing to the recorded track and the lighting setup. The lasers were integrated into the shoot to augment the scenography and add a graphic layer to the intimate production.\nRéalisation: Jean-François Sauvé\nProduction: Samuel Caron – Telescope Films\nDirection photo: Benoît-Jones Vallée\nTechnical details and implementation\n- Laser programming tailored to cinematography and shutter angles.\n- Tight sync to master audio; compositing considerations in post.\nChallenges and solutions\n- Safety and exposure control in close quarters with lasers.\n- Balancing smoke/haze density for beam visibility on camera.\nImpact and results\n- Delivered a sharp visual identity for the single; widely shared among fans.\n- Demonstrated laser aesthetics within video production constraints.\nProcess and methodology\n- Pre‑program cues and camera tests.\n- On‑shoot adjustments; post‑grade passes to seat the beams.\n---",
    "videoEmbed": "https://www.youtube.com/watch?v=E9tdt-Re3Fc"
  },
  {
    "id": "rymz-gta",
    "title": "RYMZ GTA",
    "year": null,
    "client": "Joy Ride Records / MONTREAL",
    "role": null,
    "technologies": null,
    "description": "Set-Design",
    "impact": null,
    "media": [],
    "instagram": [],
    "cover": null,
    "hasMedia": false,
    "videoEmbed": null
  },
  {
    "id": "refractions",
    "title": "REFRACTIONS",
    "year": null,
    "client": "Galerie Never Apart / MONTRÉAL - TORONTO",
    "role": null,
    "technologies": null,
    "description": "Oeuvre laser",
    "impact": null,
    "media": [],
    "instagram": [],
    "cover": null,
    "hasMedia": true,
    "fullDescription": "Refractions was a laser installation presented at Never Apart gallery during MAPP Festival 2019. Lasers were mapped onto water, materials, smoke, and a suspended glass cube, creating shifting volumes of light that changed with the viewer’s position. The setup emphasized the physical qualities of beams interacting with surfaces and atmosphere\nTechnical details and implementation\n- Multi‑laser rig with controlled scan patterns; safety interlocks.\n- Haze/atmospheric management to reveal volumes.\nChallenges and solutions\n- Alignment precision over time → rigid mounts and warm‑up protocols.\n- Audience safety in close‑range contexts → tested sightlines and barriers.\nImpact and results\n- Produced meditative, contemplative viewing; strong photographic documentation.\n- Expanded the studio’s vocabulary in pure light form.\nProcess and methodology\n- Optical tests for beam spread and reflection.\n- On‑site rehearsal of pacing and dimming curves.\n---",
    "videoEmbed": null
  },
  {
    "id": "d-s-destiny",
    "title": "D.S DESTINY",
    "year": null,
    "client": "Moment Factory / NEW-YORK",
    "role": null,
    "technologies": null,
    "description": "Custom photobooth and game development",
    "impact": null,
    "media": [
      "https://www.instagram.com/momentfactory/"
    ],
    "instagram": [
      "https://www.instagram.com/momentfactory/"
    ],
    "cover": null,
    "hasMedia": false,
    "videoEmbed": null
  },
  {
    "id": "festival-mode-design",
    "title": "FESTIVAL MODE & DESIGN",
    "year": null,
    "client": "Vincent d'Amérique / MONTRÉAL",
    "role": null,
    "technologies": null,
    "description": "Live generative visuals",
    "impact": null,
    "media": [
      "https://www.instagram.com/madfestivalofficiel/"
    ],
    "instagram": [
      "https://www.instagram.com/madfestivalofficiel/"
    ],
    "cover": null,
    "hasMedia": false,
    "videoEmbed": null
  },
  {
    "id": "colors-of-bangkok",
    "title": "COLORS OF BANGKOK",
    "year": null,
    "client": "Moment Factory / BANGKOK",
    "role": null,
    "technologies": null,
    "description": "Visuel en temps réel",
    "impact": null,
    "media": [
      "https://www.instagram.com/momentfactory/"
    ],
    "instagram": [
      "https://www.instagram.com/momentfactory/"
    ],
    "cover": null,
    "hasMedia": true,
    "fullDescription": "Central World Bangkok explored a new production pipeline where realtime software enhanced video content on the fly. Using TouchDesigner, I created effects layered over footage and tested them live during the shoot, adjusting lighting and parameters to match the conditions. The work was designed for the flagship Central World building in Bangkok, whose unusually wide screen ratio called for custom visuals and compositions. The process combined motion design with creative coding, enabling quick prototyping of aesthetics and immediate feedback on set. This collaboration showed how realtime tools could bridge the gap between production and post-production, giving motion graphic artists and coders a shared space to experiment.\nTechnical details and implementation\n- TouchDesigner‑based overlays blended with filmed footage in studio.\n- Real‑time parameter control to react to choreography.\n- Wide‑format composition for panoramic canvas.\nChallenges and solutions\n- Motion smear vs. clarity → controlled shutter and effect thresholds.\n- GPU performance under layered feedback → frame‑budget profiling.\nImpact and results\n- Created a kinetic hybrid image that celebrated movement.\n- Strengthened collaboration between recording and creative‑coding teams.\nProcess and methodology\n- On‑set experiments with dancers; immediate playback for decisions.\n- Export/LUT workflows to match post‑production look.\n---",
    "videoEmbed": "https://www.youtube.com/playlist?list=PLn6EhS40Brt9k991E_9o2zIvyKaMVZdNa"
  },
  {
    "id": "ed-fornieles-the-finiliar",
    "title": "ED FORNIELES' THE FINILIAR",
    "year": null,
    "client": "Rosenkranz Foundation / NEW-YORK",
    "role": null,
    "technologies": null,
    "description": "Visualisation de données",
    "impact": null,
    "media": [
      "https://www.instagram.com/eddfornieles/"
    ],
    "instagram": [
      "https://www.instagram.com/eddfornieles/"
    ],
    "cover": null,
    "hasMedia": true,
    "fullDescription": "The Finiliar is a collaboration with artist Ed Fornieles that brings market data to life through animated creatures. Stocks, currencies, and cryptocurrencies are linked to each Finiliar’s mood, making them grow happy or sick as values rise and fall. I developed the realtime system that fetched live financial data and drove the characters’ behavior, smoothing volatility while keeping changes expressive. The project evolved across multiple iterations, from 2D screen-based versions to a 3D installation at the Venice Biennale (Ca’ Pesaro), and has been acquired by institutional collections. By turning abstract numbers into emotional companions, The Finiliar made finance tangible, playful, and strangely empathetic.\nTechnical details and implementation\n- Realtime financial APIs feeding creature behavior; smoothing for volatility.\n- 2D and 3D pipelines; exhibition builds and web‑adjacent variants.\nChallenges and solutions\n- Unpredictable data spikes → clamping, hysteresis and narrative guardrails.\n- Designing affect that stays legible yet satirical.\nImpact and results\n- Bridged finance and pop culture; drew broad attention.\n- Institutional collection confirmed cultural traction.\nProcess and methodology\n- Data‑pipeline prototyping and fallback caching.\n- Creature rig design; mood->motion mappings.\n- Install presets for screen types and aspect ratios.\n---",
    "videoEmbed": null
  },
  {
    "id": "sound-tracer",
    "title": "SOUND TRACER",
    "year": null,
    "client": "Moment Factory / NEW YORK",
    "role": null,
    "technologies": null,
    "description": "Audiovisual instrument for kids",
    "impact": null,
    "media": [
      "https://www.instagram.com/momentfactory/"
    ],
    "instagram": [
      "https://www.instagram.com/momentfactory/"
    ],
    "cover": null,
    "hasMedia": true,
    "fullDescription": "Soundtracer was an interactive musical landscape commissioned by the Brooklyn Academy of Music. Aimed primarily at children, the installation let visitors draw with sound and light—gestures left glowing traces while speakers responded with musical phrases. Built on a multitouch surface, it supported multiple users simultaneously, turning the challenge of shared interaction into one of its biggest successes. Realtime audio generation kept the sound playful and responsive, while the interface stayed simple enough for kids to understand immediately. Durable and designed for continuous public use, Soundtracer transformed listening into an active, collaborative experience.\nTechnical details and implementation\n- Large touch/gesture surface; vision tracking to strokes.\n- Real‑time sound synthesis; projection/display for drawn paths.\nChallenges and solutions\n- Durability for continuous public use.\n- Simplicity of interface so kids grasp it instantly.\nImpact and results\n- High engagement and repeat play; accessible to non‑readers.\n- Encouraged group creativity and listening.\nProcess and methodology\n- User testing with children; simplified affordances.\n- Ruggedized components and maintainability planning.\n---",
    "videoEmbed": null
  },
  {
    "id": "renault-annual-meeting",
    "title": "RENAULT ANNUAL MEETING",
    "year": null,
    "client": "Moment Factory / PARIS",
    "role": null,
    "technologies": null,
    "description": "Capture de mouvement + Visuel en temps réel",
    "impact": null,
    "media": [
      "https://www.instagram.com/renaultgroup/"
    ],
    "instagram": [
      "https://www.instagram.com/renaultgroup/"
    ],
    "cover": null,
    "hasMedia": true,
    "fullDescription": "Renault’s Augmented Dancers brought motion capture and live performance together in Paris. Working with Compagnie Hybride, dancers shared the stage with their digital doubles—sometimes mirroring themselves, sometimes attracting swarms of particles with their movements. The project combined full 3D capture with a tight collaboration between realtime and motion graphics pipelines, ensuring fluid integration of bodies and visuals. Presented as a one-time performance for Renault’s annual gathering, it showed how interactive technologies could be applied reliably in a large-scale live event.\nTechnical details and implementation\n- Live mocap pipeline mapped to generative visuals.\n- Integration with presenter cues and show caller.\nChallenges and solutions\n- Zero‑tolerance for downtime → redundant paths and strict show control.\n- Balancing spectacle with corporate messaging.\nImpact and results\n- Raised production bar for corporate events.\n- Served as a case study for hybrid art/brand performances.\nProcess and methodology\n- Rehearsals to align messaging and beats.\n- Failover rehearsals; clean cue sheets and comms.\n---",
    "videoEmbed": null
  },
  {
    "id": "arcade-fire-infinite-content-tour",
    "title": "ARCADE FIRE INFINITE CONTENT TOUR",
    "year": null,
    "client": "Moment Factory / WORLD",
    "role": null,
    "technologies": null,
    "description": "Système de contrôle + visuel en temps réel",
    "impact": null,
    "impact-title": "ARCADE FIRE INFINITE CONTENT TOUR — Live camera ingest remixed into arena-scale LED narratives",
    "media": [
      "https://www.instagram.com/arcadefire/"
    ],
    "instagram": [
      "https://www.instagram.com/arcadefire/"
    ],
    "cover": null,
    "hasMedia": true,
    "fullDescription": "Arcade Fire – Infinite Content Tour was built around four massive LED walls that surrounded the stage. We designed a system that captured live camera feeds and transformed them in realtime, layering complex compositing effects with motion graphics to amplify key moments in the performance. The visuals converted 2D footage into 3D space and applied generative treatments that kept pace with the band’s energy. This approach made the live feed itself part of the scenography, creating an arena-scale visual identity that scaled across different venues and LED configurations.\nTechnical details and implementation\n- Live camera ingest → real‑time transforms → LED walls.\n- Look library blending generative treatments with designed assets.\nChallenges and solutions\n- Syncing multi‑wall content without visual drift.\n- Balancing spectacle with readability for fans all around the venue.\nImpact and results\n- Became a benchmark for live‑feed + motion design integration in arena shows.\n- Scalable across venues with different LED specs.\nProcess and methodology\n- Rehearsal captures to tune effect thresholds.\n- Operator macros for quick state changes during songs.\n---",
    "videoEmbed": "https://www.youtube.com/watch?v=_QDoNZKVGkk"
  },
  {
    "id": "kontinuum",
    "title": "KONTINUUM",
    "year": null,
    "client": "Moment Factory / OTTAWA",
    "role": null,
    "technologies": null,
    "description": "Parcours immersif et installations interactives",
    "impact": null,
    "impact-title": "KONTINUUM — Kilometer-long tunnel transformed with synchronized light, sound, and scans",
    "media": [
      "https://www.instagram.com/momentfactory/"
    ],
    "instagram": [
      "https://www.instagram.com/momentfactory/"
    ],
    "cover": null,
    "hasMedia": true,
    "fullDescription": "Kontinuum (2017) was a large-scale interactive installation created in Ottawa’s new Lyon Light Rail Transit (LRT) station, co-produced by Moment Factory and the Ottawa 2017 Bureau for Canada’s 150th anniversary.\nBefore the station opened to the public, the unfinished underground space was transformed into an immersive environment combining projection mapping, lighting, sound design, and interactivity. Visitors began their journey in a decommissioned cinema, where a body scan generated a unique “personal frequency” in 3D that was later integrated into the visuals. They then moved through the construction site, reimagined as a futuristic audiovisual landscape.\nThe project required close collaboration with architects and engineers, as the space was still under construction and constantly changing until launch. A multidisciplinary team adapted the stage design, technical systems, and content to fit the evolving environment.\nKontinuum attracted thousands of visitors and received international recognition, including a Merit Award for Experiential Design at the 2018 ADC Awards and an Award of Excellence in the 2017 Communication Arts Interactive Competition.\n**Technical details and implementation**\n- Large-span projection mapping system covering kilometer-long tunnel walls with synchronized content\n- Distributed audio system creating immersive soundscapes throughout the tunnel length\n- Advanced sensing technology for timed activations that respond to crowd movement and density\n- Sophisticated synchronization systems maintaining perfect timing across the entire tunnel length\n**Challenges and solutions**\n- Ensuring safety and proper egress in underground space required close coordination with city engineers\n- Maintaining synchronization and brightness over kilometer distances demanded robust technical infrastructure\n- Creating engaging experience in challenging underground environment required innovative design solutions\n**Impact and results**\n- Engaged hundreds of thousands of visitors, generating significant press coverage for civic imagination\n- Established model for pre-opening cultural activation of major infrastructure projects\n- Demonstrated how temporary art installations can transform public perception of urban infrastructure\n**Process and methodology**\n- Close collaboration with city planning and engineering teams to ensure safety and feasibility\n- Zone-by-zone commissioning and alignment to optimize experience across the entire tunnel length\n- Comprehensive testing and calibration to ensure reliable operation during high-traffic periods\n---",
    "videoEmbed": "https://www.youtube.com/watch?v=4l3oZolE3Og"
  },
  {
    "id": "orchestre-symphonique-de-montréal",
    "title": "ORCHESTRE SYMPHONIQUE DE MONTRÉAL",
    "year": null,
    "client": "Moment Factory / MONTRÉAL",
    "role": null,
    "technologies": null,
    "description": "Visuel en temps réel + Danseur",
    "impact": null,
    "media": [
      "https://www.instagram.com/osmconcerts/"
    ],
    "instagram": [
      "https://www.instagram.com/osmconcerts/"
    ],
    "cover": null,
    "hasMedia": false,
    "videoEmbed": null
  },
  {
    "id": "universal-studios",
    "title": "UNIVERSAL STUDIOS",
    "year": null,
    "client": "Moment Factory / ORLANDO",
    "role": null,
    "technologies": null,
    "description": "R&D Détection de mouvement et installations interactives",
    "impact": null,
    "media": [
      "https://www.instagram.com/universalorlando/"
    ],
    "instagram": [
      "https://www.instagram.com/universalorlando/"
    ],
    "cover": null,
    "hasMedia": false,
    "videoEmbed": null
  },
  {
    "id": "ey-innovation-realized",
    "title": "EY INNOVATION REALIZED",
    "year": null,
    "client": "Moment Factory / SAN FRANCISCO",
    "role": null,
    "technologies": null,
    "description": "Visuel en temps réel + Danseur",
    "impact": null,
    "media": [
      "https://www.instagram.com/ey_global/"
    ],
    "instagram": [
      "https://www.instagram.com/ey_global/"
    ],
    "cover": null,
    "hasMedia": false,
    "videoEmbed": null
  },
  {
    "id": "tabegami-sama",
    "title": "TABEGAMI SAMA",
    "year": null,
    "client": "Moment Factory / TOKYO",
    "role": null,
    "technologies": null,
    "description": "Exposition interactive",
    "impact": null,
    "media": [
      "https://www.instagram.com/momentfactory/"
    ],
    "instagram": [
      "https://www.instagram.com/momentfactory/"
    ],
    "cover": null,
    "hasMedia": true,
    "fullDescription": "Tabegami Sama was a multi-room interactive exhibition that explored Japanese food and culture through playful, technology-driven experiences. One room featured huge rice bowls where projection mapping and depth sensors invited visitors to experiment with gestures, revealing layered audiovisual responses. The exhibition unfolded along a narrative arc of cultural discovery, combining entertainment with information in a way that was both accessible and engaging. A major technical challenge was running four large-scale interactive walls from a single computer, solved through highly optimized realtime pipelines. Touch sensing was achieved using a 3D point cloud remapped to the installation’s geometry, allowing precise and reliable interaction\nTechnical details and implementation\n- Projection mapping onto sculptural/ritual surface.\n- Audio narrative and sensor triggers for call‑and‑response.\nChallenges and solutions\n- Respectful cultural framing and metaphor.\n- Precise mapping to complex surface geometry.\nImpact and results\n- Invited audiences to embody ritual through playful technology.\n- Expanded the practice into food/culture thematics.\nProcess and methodology\n- Motif research and consultation.\n- Prototype altar interactions; calibrate proximity sensors.\n---",
    "videoEmbed": "https://www.youtube.com/watch?v=ve8SCZnujUY"
  },
  {
    "id": "ozone",
    "title": "OZONE",
    "year": null,
    "client": "Biennale de Montréal @ Le Livart / MONTREAL",
    "role": null,
    "technologies": null,
    "description": "Installation interactive",
    "impact": null,
    "media": [],
    "instagram": [],
    "cover": null,
    "hasMedia": true,
    "fullDescription": "Ozone was a haptic audiovisual instrument built around a stretched fabric surface. Hidden sensors detected the pressure of touch, transforming each push into generative sound and visuals projected onto the material. The piece invited the public to play with intensity, discovering how physical force could shape a flowing environment of light and sound. By merging tactile feedback with realtime audiovisual generation, Ozone created a simple but powerful experience where visitors felt they could literally sculpt atmosphere with their hands.\nTechnical details and implementation\n- Real‑time environmental data inputs; normalized and smoothed.\n- Abstract visual grammars mapped to indices; projection/screen display.\nChallenges and solutions\n- Data reliability and gaps → caching and graceful degradation.\n- Avoiding literalism while keeping meaning → legend and pacing.\nImpact and results\n- Encouraged reflective engagement with climate metrics.\n- Demonstrated poetic data art without didactic graphs.\nProcess and methodology\n- Data schema design and unit handling.\n- A/B tests for visual comprehensibility at distance.\n---",
    "videoEmbed": "https://vimeo.com/118873355"
  },
  {
    "id": "red-hot-chili-peppers-getaway-tour",
    "title": "RED HOT CHILI PEPPERS GETAWAY TOUR",
    "year": null,
    "client": "Moment Factory / WORLD",
    "role": null,
    "technologies": null,
    "description": "Visuel en temps réel + Éclairage cinétique",
    "impact": null,
    "media": [
      "https://www.instagram.com/chilipeppers/"
    ],
    "instagram": [
      "https://www.instagram.com/chilipeppers/"
    ],
    "cover": null,
    "hasMedia": true,
    "fullDescription": "Red Hot Chili Peppers – Getaway Tour featured a kinetic lighting ceiling and stage visuals controlled in real time. We designed a custom tool in TouchDesigner to generate fluid movement across motorized lights while sending RGB values to video content. Audioreactive components responded to the music, while live feed effects were composited with motion graphics to amplify the performance. By introducing interactive sensors into the arena context, the project helped push the live show industry toward trusting real-time, responsive systems. The result was a portable and flexible visual package that raised the intensity of the tour.\nTechnical details and implementation\n- TouchDesigner effects on camera feeds; LED wall delivery.\n- Color and rhythm cues coordinated with lighting director.\nChallenges and solutions\n- Venue variability and rapid load‑ins.\n- Keeping compositions legible amid fast edits and lights.\nImpact and results\n- Raised production intensity without drowning the players.\n- Portable package adapted across continents.\nProcess and methodology\n- Template looks per song; notes for day‑of changes.\n- On‑the‑fly operator macros for solos/transitions.\n---",
    "videoEmbed": null
  },
  {
    "id": "nova-lumina",
    "title": "NOVA LUMINA",
    "year": null,
    "client": "Moment Factory / CHANDLER",
    "role": null,
    "technologies": null,
    "description": "Parcours immersif et installation interactive",
    "impact": null,
    "impact-title": "NOVA LUMINA — Night trail where visitors appear as holograms within a starry forest",
    "media": [
      "https://www.instagram.com/momentfactory/"
    ],
    "instagram": [
      "https://www.instagram.com/momentfactory/"
    ],
    "cover": null,
    "hasMedia": true,
    "fullDescription": "Nova Lumina (2016) is a permanent nighttime walking trail created in Parc du Bourg de Pabos, Québec, as part of the Lumina series by Moment Factory.\nAlong the 1.5-kilometre coastal path, visitors are invited to accompany a fallen star back to the night sky. A key innovation was the use of merged depth sensors to transform visitors into participants in holographic projections, including them in a 3D space and a forest made of stars. These interactive moments responded to music and lighting cues, weaving personal presence directly into the story.\nThis project marked an innovative use of sensors in an outdoor forest environment, expanding the possibilities of blending natural landscapes with interactive technology.\nTechnical details and implementation\n- Distributed outdoor projection/lighting and spatialized sound.\n- Narrative sequencing across multiple stations.\nChallenges and solutions\n- Weatherproof operation over long seasons.\n- Managing large nightly visitor throughput.\nImpact and results\n- Became a cultural tourism anchor for the region.\n- Offered families a shared media art experience in nature.\nProcess and methodology\n- Trail design and station storyboards.\n- Lighting zones, power and maintenance cycles.\n---",
    "videoEmbed": "https://vimeo.com/179048368"
  },
  {
    "id": "aura",
    "title": "AURA",
    "year": null,
    "client": "MONTRÉAL EN LUMIÈRES / MONTRÉAL",
    "role": null,
    "technologies": null,
    "description": "Espace extérieur immersif. Projection architecturale",
    "impact": null,
    "impact-title": "AURA — TRAVERSÉE IMMERSIVE — Nuit Blanche square reimagined with projection, fiber trees, and touch-reactive mushrooms",
    "media": [
      "https://www.instagram.com/momentfactory/"
    ],
    "instagram": [
      "https://www.instagram.com/momentfactory/"
    ],
    "cover": null,
    "hasMedia": true,
    "fullDescription": "Aura — Traversée immersive (2016) was a one-night installation presented during Nuit Blanche à Montréal, transforming Place Pasteur on rue Saint-Denis into an illuminated forest.\nThe project combined projection mapping on the clocher of Place Pasteur with an environment built from fibre-optic light trees and interactive mushrooms that reacted to touch. As visitors walked through, their presence and gestures triggered shifts in light and atmosphere, gradually revealing a lush, animated landscape emerging from the winter setting.\nThis was my first large-scale projection using TouchDesigner, where I developed the interactive system that linked participant presence to the evolving visuals. Participation was central: each person’s presence “fed” the installation with light, making the forest denser and more alive over time.\n**Technical details and implementation**\n- Large-scale interior projection mapping system designed specifically for the basilica's complex architecture\n- Multi-channel sound system creating immersive audio that complements the visual experience\n- Permanent infrastructure with careful mounting and alignment to preserve historic materials\n- Sophisticated content creation pipeline that respects the building's sacred character\n**Challenges and solutions**\n- Respecting sacred space and historic materials required careful planning and preservation-safe installation practices\n- Creating rendering that flatters ornate surfaces without glare demanded precise projection mapping and content design\n- Balancing artistic vision with religious sensitivity required ongoing collaboration with church authorities\n**Impact and results**\n- Achieved immense public popularity and became a long-running cultural program\n- Successfully helped reposition the Basilica as a major cultural venue in Montreal\n- Demonstrated how technology can enhance rather than diminish the spiritual experience of sacred spaces\n**Process and methodology**\n- Comprehensive 3D scanning and content creation specifically authored to the basilica's unique geometry\n- Iterative after-hours calibration and testing to ensure optimal experience without disrupting services\n- Development of preservation-safe installation practices that protect historic materials and surfaces\n---",
    "videoEmbed": "https://tv.uqam.ca/video/aura-traversee-immersive/"
  },
  {
    "id": "en-route-vers-les-jutras",
    "title": "EN ROUTE VERS LES JUTRAS",
    "year": null,
    "client": "Société des arts technologiques / MONTRÉAL",
    "role": null,
    "technologies": null,
    "description": "Vjing",
    "impact": null,
    "media": [
      "https://www.instagram.com/sat_montreal/"
    ],
    "instagram": [
      "https://www.instagram.com/sat_montreal/"
    ],
    "cover": null,
    "hasMedia": false,
    "videoEmbed": null
  },
  {
    "id": "basel-2022",
    "title": "BASEL 2022 PROJECTS",
    "year": "2022",
    "client": "Rafael Lozano-Hemmer / Basel",
    "role": null,
    "technologies": null,
    "description": "Thermal imaging and interactive installations",
    "impact": null,
    "media": [
      "https://www.instagram.com/lozanohemmer/"
    ],
    "instagram": [
      "https://www.instagram.com/lozanohemmer/"
    ],
    "cover": null,
    "hasMedia": true,
    "fullDescription": "A focused art‑fair presentation including editions of Thermal Drift and related works, tailored to fair constraints while preserving impact.\nTechnical details and implementation\n- Editioned builds optimized for booth light and viewing distance.\n- Compact, reliable show control for long open hours.\nChallenges and solutions\n- High traffic and transient viewing → immediate legibility.\n- Tight install/strike windows and shipping constraints.\nImpact and results\n- Connected works with collectors and curators; led to sales.\n- Expanded presence in the international fair circuit.\nProcess and methodology\n- Pre‑packaged crates and checklists.\n- In‑booth calibration and heat‑management for equipment.\n---",
    "videoEmbed": null
  },
  {
    "id": "halte-0-sepaq-iles-de-boucherville",
    "title": "HALTE 0 SÉPAQ ÎLES-DE-BOUCHERVILLE",
    "year": null,
    "client": "SÉPAQ / MONTRÉAL",
    "role": null,
    "technologies": null,
    "description": "Architecture + Interactivité",
    "impact": null,
    "media": [],
    "instagram": [],
    "cover": null,
    "hasMedia": true,
    "fullDescription": "Halte 0, Parc national des Îles-de-Boucherville (2018) is a permanent interactive installation located at the park’s information desk. Designed as an immersive wall, it reveals itself as visitors pass by, giving access to an interactive map of the park’s new trails and natural features. The wall is both informative and playful, aiming to intrigue, entertain, and raise awareness about environmental issues connected to the site.\nThe installation was realized with lidar sensors and TouchDesigner, supported by a custom content management system that allows the piece to evolve over time. Interactivity is seamlessly integrated into the architecture, creating a durable long-term solution suited to the public setting.\nI contributed to the project through interaction concept, creative coding, technology integration, and technical direction, in collaboration with TUX (production), Champagne Club Sandwich (creative direction), and Ottomata (interactive direction, coding, and technical direction).\nTechnical details and implementation\n- Projection mapping onto terrain/structures; nature‑recording‑infused sound.\n- Weatherized enclosures; long‑throw projection across water.\nChallenges and solutions\n- Outdoor unpredictability → robust housings, anti‑condensation, remote monitoring.\n- Visitor safety and path lighting while preserving darkness.\nImpact and results\n- Expanded park programming; brought new audiences after dark.\n- A gentle, respectful intervention in a protected landscape.\nProcess and methodology\n- Site surveys; throw tests across water; power routing.\n- Evening‑of adjustments for wind, haze, and wildlife activity.\n---",
    "videoEmbed": "https://ottomata.com/en/halte-0-sepaq-iles-de-boucherville"
  },
  {
    "id": "interactive-basketball",
    "title": "INTERACTIVE BASKETBALL",
    "year": null,
    "client": "Moment Factory / MONTRÉAL",
    "role": null,
    "technologies": null,
    "description": "High Speed Tracking - Using Panasonic high speed projector and tracking system",
    "impact": null,
    "media": [],
    "instagram": [],
    "cover": null,
    "hasMedia": false,
    "videoEmbed": null
  },
  {
    "id": "my-morning-jacket",
    "title": "MY MORNING JACKET",
    "year": null,
    "client": "YouTube / WORLD",
    "role": null,
    "technologies": null,
    "description": "Video frame extraction from live performance",
    "impact": null,
    "media": [],
    "instagram": [],
    "cover": null,
    "hasMedia": false,
    "videoEmbed": null
  }
];
